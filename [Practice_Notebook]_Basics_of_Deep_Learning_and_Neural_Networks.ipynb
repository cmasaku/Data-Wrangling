{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BT7EbxRqsrwP",
        "DcyeAslXt11E",
        "aq52sCL4t7BI",
        "nKDL-nmut98Z",
        "pExozwHouAKA",
        "4PV0KUr7-Wjb",
        "CwTGVmItkHsA",
        "ffP-HhLxuBi1",
        "ZTjlaLg4AsIi",
        "YYZoVkgwAsI4",
        "2zfDjd4HAsJT",
        "R4OF9HnrAsJn",
        "6JwRbFhZAsKH",
        "o_Ytd4guAsKI",
        "JIsPvb1cAsK6",
        "nJ9r88PwhuSU",
        "_B2OfF79AsLM",
        "sLisP_91s2mM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmasaku/Udacity-Projects/blob/main/%5BPractice_Notebook%5D_Basics_of_Deep_Learning_and_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgtpGH1XsplI"
      },
      "source": [
        "# Practice Notebook: Basics of Deep Learning and Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT7EbxRqsrwP"
      },
      "source": [
        "## Importing Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh1tDQE01Y6P"
      },
      "source": [
        "# Importing the required libraries\n",
        "# ---\n",
        "#\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set dataframe column width as max\n",
        "# ---\n",
        "#\n",
        "pd.set_option('display.max.columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Show visualisation in the notebook\n",
        "# ---\n",
        "#\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbO22v-2s9C1"
      },
      "source": [
        "## Example: Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iABEt0jvs4ny"
      },
      "source": [
        "## Example 1\n",
        "# ---\n",
        "# Create a classification model using neural networks that will make\n",
        "# a prediction on whether a person survived the titanic disaster.\n",
        "# ---\n",
        "# Train Dataset = https://bit.ly/31azYjb\n",
        "# Test Dataset = https://bit.ly/2XmmAYe\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcyeAslXt11E"
      },
      "source": [
        "### Step 1: Data Importation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFF31bfeuIOI"
      },
      "source": [
        "# Loading and previewing the train dataset\n",
        "# ---\n",
        "#\n",
        "df = pd.read_csv('https://bit.ly/3d1Te88')\n",
        "df.sample(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq52sCL4t7BI"
      },
      "source": [
        "### Step 2: Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu8rl3KnH5xg"
      },
      "source": [
        "# finding unique value for target variable\n",
        "df.Survived.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsacnWa3uIqT"
      },
      "source": [
        "# checking the datasets' shape\n",
        "print(\"Dataset shape:\", df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7bSTlQJ2ep1"
      },
      "source": [
        "# checking data types of Train\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKDL-nmut98Z"
      },
      "source": [
        "### Step 3: Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDeueSB02pCX"
      },
      "source": [
        "# checking for missing data in Train\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sw163YePaJY"
      },
      "source": [
        "# Selecting our features\n",
        "# ---\n",
        "# The method we will use here will be to create a list containing\n",
        "# all column names and to remove our target variable name then\n",
        "# selecting the features with the feature names in list.\n",
        "# ---\n",
        "#\n",
        "properties = list(df.columns.values)\n",
        "properties.remove('Survived')\n",
        "X = df[properties]\n",
        "\n",
        "# Selecting our target variable\n",
        "# ---\n",
        "#\n",
        "y = df['Survived']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zC-jvw4V4mF"
      },
      "source": [
        "# Splitting our dataset\n",
        "# ---\n",
        "#\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pExozwHouAKA"
      },
      "source": [
        "### Step 4: Data Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PV0KUr7-Wjb"
      },
      "source": [
        "##### Creating Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imI39pwM-7ct"
      },
      "source": [
        "We will create a base model and compare its performance with our Artificial Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLzgHbCtuJiF"
      },
      "source": [
        "# For our base model, we will use the Random Forest Classifier\n",
        "# ---\n",
        "#\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Creating our base model instance\n",
        "# ---\n",
        "#\n",
        "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Fitting our base model\n",
        "# ---\n",
        "#\n",
        "random_forest_classifier = random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Performing our prediction with the base model\n",
        "# ---\n",
        "#\n",
        "y_prediction = random_forest_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jNq0htPhlRM"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Random Forest Classifier\", accuracy_score(y_prediction, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwTGVmItkHsA"
      },
      "source": [
        "##### Creating our Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhqr4b0gIDXo"
      },
      "source": [
        "# We first import the keras library which will help us build an Artificial Neural Network\n",
        "# ---\n",
        "# Artificial Neural Networks in Keras are defined as a sequence\n",
        "# of layers which would be input, hidden and output-layers.\n",
        "# Keras takes a group of sequential layers and stacks them together into a single model.\n",
        "# We also add dropout dropout regularization functions to the input and\n",
        "# hidden layers in order to prevent overfitting.\n",
        "# ---\n",
        "#\n",
        "import keras\n",
        "from keras.models import Sequential     # Used to initialize the Artificial Neural Network\n",
        "from keras.layers import Dense          # Used to build the hidden Layers\n",
        "from keras.layers import Dropout        # Used to prevent overfitting\n",
        "\n",
        "\n",
        "# We start by creating an instance of Artificial Neural Network as shown\n",
        "# ---\n",
        "# Our classifier will return is an integer value, 0 or 1.\n",
        "# ---\n",
        "#\n",
        "classifier = Sequential()\n",
        "\n",
        "# Then adding the input layer and the first hidden layer with dropout function.\n",
        "# The input layer would be the first layer of our Artificial Neural Network.\n",
        "# ---\n",
        "# ->  units = 100         : We specify the no. of units (neurons) our connected layer\n",
        "#                           (the hidden layer attached) is going to have.\n",
        "#                           Normally, you'd have to try different values as your no. of neurons\n",
        "#                           per layer through trial and error.\n",
        "# ->  input_dim = 4       : We make use of input_dim to pass the dimensions of the input data to the Dense layer.\n",
        "#                           This would be the no. of features in our dataset.\n",
        "# ->  activation = 'relu' : Within our hidden layers we use the relu function as it yields a satisfactory result most of the time.\n",
        "#                           However, we can also experiment with other activation functions.\n",
        "# Lastly, we add a dropout regularization function that will prevent our ANN from overfitting.\n",
        "# - We should always use a dropout rate between 20% and 50%.\n",
        "#   In our case will dropped 30% of the input data to avoid overfitting.\n",
        "#   The seed is set to 2 in order to get reproducible results.\n",
        "#   If we don't specify this each model's outcome would be different.\n",
        "# ---\n",
        "#\n",
        "classifier.add(Dense(units = 100, input_dim = 4, activation = 'relu'))\n",
        "classifier.add(Dropout(0.3, seed = 2))\n",
        "\n",
        "# Adding a second hidden layer\n",
        "# ---\n",
        "# The second layer is similar, we dont need to specify input dimension\n",
        "# as we have defined the model to be sequential so keras will automatically\n",
        "# consider input dimension to be same as the output of last layer i.e 4.\n",
        "# ---\n",
        "#\n",
        "classifier.add(Dense(units = 100, activation = 'relu'))\n",
        "classifier.add(Dropout(0.3, seed = 2))\n",
        "\n",
        "# Adding an output layer\n",
        "# ---\n",
        "# We set units = 1, because for our output, our ANN to return a single integer value, either 0 or 1.\n",
        "# We also use the sigmoid function which maps the values between 0 and 1.\n",
        "# ---\n",
        "#\n",
        "classifier.add(Dense(units = 1, activation = \"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUb9iGDZIH4Y"
      },
      "source": [
        "# Finally Compile our ANN\n",
        "# ---\n",
        "# By compiling, we are simply configuring the model for training\n",
        "# ---\n",
        "# optimizer = 'adam'  :          The optimizer controls the learning rate throughout training,\n",
        "#                                i.e. how fast the optimal weights for the model are calculated.\n",
        "#                                A smaller learning rate would lead to more accurate weights (up to a certain point),\n",
        "#                                but the time it takes to compute the weights will be longer.\n",
        "#                                'adam' is generally a good optimizer to use for many cases.\n",
        "# loss = 'binary_crossentropy':  This defines how we get closer to our loss.\n",
        "#                                In our case, since our output is binary, we use ‘binary_crossentropy’.\n",
        "#                                For multi-class classification we can use 'categorical_crossentropy; as our loss.\n",
        "#                                This would evaluate how well our ANN models the given data\n",
        "# Lastly, we choose accuracy as our evaluation metric.\n",
        "# ---\n",
        "#\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-R2dBH4IQd0"
      },
      "source": [
        "# Training our model\n",
        "# ---\n",
        "# Lets now train our model using our dataset.\n",
        "# Here learning is an iterative process and we tell the model to\n",
        "# go through our training dataset to learn as much as it can from it\n",
        "# ---\n",
        "# Training occurs over epochs and each epoch is split into batches.\n",
        "# - Epoch: One pass through all of the rows in the training dataset.\n",
        "# - Batch: One or more samples considered by the model within an epoch before weights are updated.\n",
        "#          The higher the batch size, the more memory space we'll need.\n",
        "# These configurations can be chosen experimentally by trial and error.\n",
        "# We want to train the model enough so that it learns a good (or good enough)\n",
        "# mapping of rows of input data to the output classification.\n",
        "# The model will always have some error, but the amount of error will level out\n",
        "# after some point for a given model configuration.\n",
        "# This point would be called as the point of model convergence.\n",
        "# ---\n",
        "# NB: We are using y_train set that underwent one hot encoding.\n",
        "# ---\n",
        "#\n",
        "classifier.fit(X_train, y_train, epochs = 300, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbZwVLafQGvX"
      },
      "source": [
        "# Model Evaluation\n",
        "# ---\n",
        "# We then evaluate our model for test set by checking the accuracy\n",
        "# ---\n",
        "# We can improve our model by:\n",
        "# 1. Optimizing the epochs.\n",
        "# 2. Optimizing the number of layers.\n",
        "# 3. Optimizing the number of nodes per layer.\n",
        "# ---\n",
        "#\n",
        "loss, accuracy = classifier.evaluate(X_test, y_test)\n",
        "print('ANN Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP-HhLxuBi1"
      },
      "source": [
        "### Step 5: Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkupyOl_uKFI"
      },
      "source": [
        "# Question\n",
        "# ---\n",
        "# Say we wanted to determine whether a 40 year woman in Class 4 and Paid 30\n",
        "# survived the titanic we can make this prediction by\"\n",
        "# ---\n",
        "#\n",
        "new_value = np.array([[4, 0, 40, 30]])\n",
        "\n",
        "# Making our prediction\n",
        "# ---\n",
        "# We use the predict() method to get the predicted probabilities for each class.\n",
        "#\n",
        "predicted_probabilities = classifier.predict(new_value)\n",
        "\n",
        "# We use np.argmax() to find the index of the class with the highest probability, which is essentially the predicted class.\n",
        "# This should give you the desired output indicating whether the woman survived or not.\n",
        "# ---\n",
        "#\n",
        "predicted_class = np.argmax(predicted_probabilities)\n",
        "print(predicted_class)\n",
        "\n",
        "# The output would be 0 which, would mean the woman did not survive."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTjlaLg4AsIi"
      },
      "source": [
        "## Example: Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asgqGe8jAsIu"
      },
      "source": [
        "## Example 1\n",
        "# ---\n",
        "# Create a regression model using artificial neural networks\n",
        "# to predict the weight of fish given the following dataset.\n",
        "# ---\n",
        "# Dataset = http://bit.ly/MRFishDataset\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYZoVkgwAsI4"
      },
      "source": [
        "### Step 1: Data Importation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF16YkOKAsI5"
      },
      "source": [
        "# Loading and previewing the train dataset\n",
        "# ---\n",
        "#\n",
        "fish_df = pd.read_csv('http://bit.ly/MRFishDataset')\n",
        "fish_df.sample(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zfDjd4HAsJT"
      },
      "source": [
        "### Step 2: Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlk-BoSCFw44"
      },
      "source": [
        "# Previewing the statistical summary of our dataset\n",
        "#\n",
        "fish_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX5nUdhGGIVD"
      },
      "source": [
        "# Performing Exploratory Analysis\n",
        "# ---\n",
        "# This time we will plot a correlation matrix, to determine the relationships between the different variables.\n",
        "# This matrix will give us a sense of how well the variables are correlated. By this we mean, whether an\n",
        "# increase or decrease in variable affects the other variable.\n",
        "# To break this down further, the matrix will provide us with values between -1 and 1. If the value between\n",
        "# two variables is closer to 1 i.e. > 0.5, then it means the variables are strongly correlated, have a positive linear\n",
        "# relationship and it also means that as one value increases the other increases.\n",
        "# On the other hand, of the value is less than -0.5, it would mean that the variables are strongly correlated but\n",
        "# have a negative linear relationship.\n",
        "# If the value is 0 or < -0.5 or < 0.5 it means that the variables don't have any relationship with each other.\n",
        "# ---\n",
        "# This type of visualisation can help us examine an assumption of linear regression;\n",
        "# relationship of predicor variables with the response variable.\n",
        "# ---\n",
        "#\n",
        "corrMatrix = fish_df.corr()\n",
        "corrMatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHfFeq9cGIVF"
      },
      "source": [
        "# We can plot a visualisation of the matrix for better clarity\n",
        "# ---\n",
        "#\n",
        "import seaborn as sns\n",
        "\n",
        "# We define how big we want our visualisation\n",
        "#\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Creating our visualisation\n",
        "#\n",
        "sns.heatmap(corrMatrix, annot = True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4OF9HnrAsJn"
      },
      "source": [
        "### Step 3: Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q9ipuxEGE32"
      },
      "source": [
        "# Selecting our feature and response variables\n",
        "# ---\n",
        "#\n",
        "X = fish_df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = fish_df['Weight']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-ytfcVLAsJ-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Performing our split\n",
        "# ---\n",
        "#\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JwRbFhZAsKH"
      },
      "source": [
        "### Step 4: Data Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Ytd4guAsKI"
      },
      "source": [
        "#### Creating the Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ob19JIAsKJ"
      },
      "source": [
        "We will create a base model and compare its performance with our Artificial Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3NB25BVAsKL"
      },
      "source": [
        "# For our base model, we will use the Random Forest Classifier\n",
        "# ---\n",
        "#\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Creating instances of our models\n",
        "# ---\n",
        "#\n",
        "decision_tree_regressor = DecisionTreeRegressor(random_state=0)\n",
        "\n",
        "# Training our machine learning algorithms\n",
        "# ---\n",
        "#\n",
        "decision_tree_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "# ---\n",
        "#\n",
        "decision_tree_pred = decision_tree_regressor.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQzlVpprAsKV"
      },
      "source": [
        "from sklearn import metrics\n",
        "print('Decision Tree: Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, decision_tree_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIsPvb1cAsK6"
      },
      "source": [
        "#### Creating our Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chNAzqK9AsK7"
      },
      "source": [
        "# Importing our library and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Instantiating our ANN regressor\n",
        "regressor = Sequential()\n",
        "\n",
        "# Adding input layer\n",
        "regressor.add(Dense(units = 10, input_dim = 5, activation = 'relu'))\n",
        "regressor.add(Dropout(0.3, seed = 2))\n",
        "\n",
        "# Adding a second hidden layer\n",
        "regressor.add(Dense(units = 10, activation = 'relu'))\n",
        "regressor.add(Dropout(0.3, seed = 2))\n",
        "\n",
        "# Adding an output layer\n",
        "# ---\n",
        "# Our network will end with a single unit 1, and doesn’t include an activation.\n",
        "# This would be the case for regression, where we are trying to predict a single continuous value.\n",
        "# ---\n",
        "regressor.add(Dense(units = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "827UoOpjAsK_"
      },
      "source": [
        "# Finally Compiling our ANN\n",
        "# ---\n",
        "# We use the rsmprop as our optimization algorithm\n",
        "# and mse as the loss function which is popular mse as the loss function.\n",
        "# We also use the Mean Absolute Error (MAE) as a metric.\n",
        "# ---\n",
        "#\n",
        "regressor.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HWHqv69AsLD"
      },
      "source": [
        "# Training our model\n",
        "# ---\n",
        "#\n",
        "regressor.fit(X_train, y_train, epochs=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTgresTcAsLI"
      },
      "source": [
        "# Model Evaluation on Test Data\n",
        "# ---\n",
        "# We use the evaluate() function which will calculate the values\n",
        "# of the metrics we chose when we compiled the model.\n",
        "# ---\n",
        "# - MAE (Mean Absolute Error) quantifies how close predictions are to the eventual outcomes.\n",
        "# - MSE (Mean Squared Error) measures the average of the squares of the errors or deviations.\n",
        "#   The closer to 0, the better. For our case, we will also use the RMSE.\n",
        "# ---\n",
        "#\n",
        "mse_value, mae_value = regressor.evaluate(X_test, y_test)\n",
        "\n",
        "print('Mean squared error: ', mse_value)\n",
        "print('Mean absolute error: ', mae_value)\n",
        "print('Root Mean squared error: ', np.sqrt(mse_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNHhdwuQr1hs"
      },
      "source": [
        "From our MAE, the regressor on average predicted 240.39 above or below the actual values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ9r88PwhuSU"
      },
      "source": [
        "#### Explaining our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5kfJ-2jFZ_"
      },
      "source": [
        "In a case where we need to explain what are the major components used by our model to perform its prediction, we can use the **SHAP** library. This allows us to create a summary of our features and its impact on the model output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1dN-3M9h8sr"
      },
      "source": [
        "# Installing shap\n",
        "# ---\n",
        "#\n",
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cjdEp-nhxF1"
      },
      "source": [
        "import shap\n",
        "shap.initjs()\n",
        "\n",
        "explainer = shap.KernelExplainer(regressor, X_train.values)\n",
        "shap_values = explainer.shap_values(X_test.values)\n",
        "\n",
        "# Plot summary_plot as barplot\n",
        "# ---\n",
        "#\n",
        "shap.summary_plot(shap_values, X_test, plot_type='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZH_gmmIkjJR"
      },
      "source": [
        "The summary plot shows the most important features and the magnitude of their impact on the model. We can observe that Length2 contributed the most during prediction followed by Length3, Length1, Height and Width."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B2OfF79AsLM"
      },
      "source": [
        "### Step 5: Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw3DH7GUAsLN"
      },
      "source": [
        "# Making predictions\n",
        "# ---\n",
        "# We make predictions using our ANN by passing an array\n",
        "# of feature values for our new prediction.\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Question:\n",
        "# ---\n",
        "# Say we wanted to determine the weight of fish with the following dimensions:\n",
        "# 1. Length1: 30.9\n",
        "# 2. Length2: 33.5\n",
        "# 3. Length3: 38.6\n",
        "# 4. Height:  15.6330\n",
        "# 5. Width:   5.1338\n",
        "# ---\n",
        "#\n",
        "new_value = np.array([[30.9, 33.5, 38.6, 25.6330, 5.1338]])\n",
        "\n",
        "# Making our prediction\n",
        "# ---\n",
        "#\n",
        "print(regressor.predict(new_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLisP_91s2mM"
      },
      "source": [
        "## <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDKOL1yjs1_g"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Create an artificial neural networks classification model that\n",
        "# predicts insurance costs given the following dataset.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/30GtDfO\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgc3gbIAsjS9"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Build a neural network to predict insurance costs given the following dataset.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/InsuranceDS\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}